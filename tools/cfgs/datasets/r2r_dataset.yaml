Dataset:
  DATA_DIR: "/home/zlin/vln/turning/VLN-DUET"
  IMG_DIR: "/media/zlin/2CD830B2D8307C60/Dataset/features/mp3d_raw_images"

  # navigableLocations
  MP3D_NAV: "data/mp3d/scan_viewpoint_nav_dict.pkl"

  # Specify the dataset name to useR2R
  SOURCE: ['R2R', 'SOON', 'REVERIE', 'CVDN'] # 'R2R', 'SOON', 'REVERIE', 'CVDN'
  SOON:
    DIR: "data/SOON/annotations/duet" # train.json
    SPLIT: {
      "train": "train_enc_pseudo_obj_label.jsonl",
      "val_seen": "val_unseen_instrs_enc_pseudo_obj_label.jsonl",
      "val_unseen": "val_unseen_house_enc_pseudo_obj_label.jsonl",
      "test": "None"
    }
#    DIR: "data/SOON/annotations/iccv21_new_released" # train.json
#    SPLIT: {
#      "train": "train.json",
#      "val_seen": "val_unseen_house.json",
#      "val_unseen": "val_unseen_instrs.json",
#      "test": "test_v2.json"
#    }

  FR2R:
    DIR: "data/Fine-Grained-R2R/data" # FGR2R_train.json
    SPLIT: {
      "train": "FGR2R_train.json",
      "val_seen": "FGR2R_val_seen.json",
      "val_unseen": "FGR2R_val_unseen.json",
      "test": "FGR2R_test.json"
    }

  R2R:
    DIR: "data/R2R/annotations"
    SPLIT: {
      "train": "R2R_train_enc.json",
      "val_seen": "R2R_val_seen_enc.json",
      "val_unseen": "R2R_val_unseen_enc.json",
      "test": "R2R_test_enc.json"
    }

  REVERIE:
    DIR: "data/REVERIE"
    SPLIT: {
      "train": "REVERIE_train.json",
      "val_seen": "REVERIE_val_seen.json",
      "val_unseen": "REVERIE_val_unseen.json",
      "test": "REVERIE_test.json"
    }

  EQA:
    DIR: "data/mp3d/mp3d_eqa_dict.json"

  CVDN:
    DIR: "data/CVDN"
    SPLIT: {
      "train": "train.json",
      "val_seen": "val_seen.json",
      "val_unseen": "val_unseen.json",
      "test": "test_cleaned.json"
    }
#    SPLIT: {
#      "train": "train_enc.json",
#      "val_seen": "val_seen_enc.json",
#      "val_unseen": "val_unseen_enc.json",
#      "test": "test_enc.json"
#    }

  Img_Features_File_Map: {
    # from HM3D-AutoVLN
    "timm_vitb16": "datasets/R2R/features/view_timm_imagenet_vitb16",
    # from VLN-DUET/SOON pretrain&fine-tuning
    "vit_imagenet": "datasets/R2R/features/pth_vit_base_patch16_224_imagenet.hdf5"
  }
  image_feat_size: 768
  image_prob_size: 1000

  With_Object_Feats: False
  Object_Features_File_Map: {
    # from VLN-DUET/SOON pretrain&fine-tuning
    "butd_SOON": "datasets/SOON/features/filtered_butd_bboxes.hdf5",
    # from HM3D-AutoVLN/SOON
    "timm_vitb16_SOON": "datasets/SOON/features/obj2d_ade20k_timm_vitb16",
    # from HM3D-AutoVLN/REVERIE
    "timm_vitb16_REVERIE": "datasets/REVERIE/features/obj_gtmax_timm_imagenet_vitb16",
    # from VLN-DUET/REVERIE pretrain&fine-tuning
    "vit_base_REVERIE": "datasets/REVERIE/features/obj.avg.top3.min80_vit_base_patch16_224_imagenet.hdf5"
  }
  obj_feat_size: 2048
  obj_prob_size: 1601
  max_objects: 100

  # tokenizer.eos_token
  tokenizer:
    eos_token: '</s>'
    max_length: 512
    prompt:
      type: "flamingo"
      task_type: "short"
      image_id: True
      template: {
        "common": "{task_description}Environment:{environment}{question}{answer}{tokenizer_eos_token}",
        "flamingo": "{task_description}Environment:{environment}{question}{answer}{endofchunk}{tokenizer_eos_token}",
      }

  vision_encoder:
    image_size: [224,224]
    path: "ViT-L-14"
    pretrained: "openai"

Inference:
  max_new_tokens: 20 # max generation length when inference
  num_beams: 1 # =1: greedy generation
  length_penalty: -2.0

Agent:
  max_action_len: 15
  task_desc_type: "short"